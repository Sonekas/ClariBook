# Simplificador de EPUB com LLM Local

Um programa web que permite simplificar livros EPUB usando modelos de linguagem locais (open-source) em vez de APIs externas.

## üöÄ **Novidades da Vers√£o Local**

### **Modelos Suportados**
- **Mistral-7B-Instruct-v0.3**: Modelo principal (requer GPU/CPU potente)
- **OpenHermes-2.5-Mistral-7B**: Modelo alternativo otimizado
- **Simplificador Baseado em Regras**: Fallback para ambientes limitados

### **Vantagens do LLM Local**
- ‚úÖ **Privacidade Total**: Nenhum dado sai do seu computador
- ‚úÖ **Sem Custos de API**: N√£o precisa pagar por tokens
- ‚úÖ **Offline**: Funciona sem conex√£o com internet
- ‚úÖ **Controle Total**: Customize prompts e par√¢metros
- ‚úÖ **Sem Limites**: Processe quantos livros quiser

## üìã **Requisitos de Sistema**

### **M√≠nimos (Simplificador Baseado em Regras)**
- **RAM**: 4GB
- **CPU**: Qualquer processador moderno
- **Armazenamento**: 2GB livres
- **Python**: 3.11+

### **Recomendados (LLM Local)**
- **RAM**: 16GB+ (32GB ideal)
- **GPU**: NVIDIA com 8GB+ VRAM (opcional mas recomendado)
- **CPU**: 8+ cores
- **Armazenamento**: 20GB+ livres (para modelos)
- **Python**: 3.11+

### **Ideais (Performance M√°xima)**
- **RAM**: 32GB+
- **GPU**: NVIDIA RTX 4090 ou similar (24GB VRAM)
- **CPU**: 16+ cores
- **Armazenamento**: SSD com 50GB+ livres

## üõ† **Instala√ß√£o**

### **1. Preparar Ambiente**
```bash
# Clonar/baixar projeto
cd simplificador-epub

# Criar ambiente virtual
python -m venv venv

# Ativar ambiente virtual
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate
```

### **2. Instalar Depend√™ncias**
```bash
# Instalar depend√™ncias b√°sicas
pip install -r requirements.txt

# Para GPU NVIDIA (opcional, melhora performance)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### **3. Configurar Modelo**

O sistema tentar√° carregar os modelos na seguinte ordem:

1. **Mistral/OpenHermes** (se hardware suportar)
2. **Simplificador baseado em regras** (fallback)
3. **OpenAI API** (se configurada)

Para for√ßar uso de modelo espec√≠fico, edite `src/routes/epub_processor.py`.

### **4. Executar Aplica√ß√£o**
```bash
python src/main.py
```

Acesse: `http://localhost:5000`

## üéØ **Como Usar**

### **Upload e Processamento**
1. **Envie arquivo EPUB** via interface web
2. **Escolha n√≠vel de simplifica√ß√£o**:
   - **N√≠vel 1**: Muito simples (frases curtas, vocabul√°rio b√°sico)
   - **N√≠vel 2**: M√©dio (equilibrio entre simplicidade e qualidade)
   - **N√≠vel 3**: Comum (pequenos ajustes para fluidez)
3. **Aguarde processamento** (pode levar v√°rios minutos)
4. **Baixe resultado** quando conclu√≠do

### **Monitoramento**
- Progresso em tempo real na interface
- Logs detalhados no terminal
- Status de cada cap√≠tulo processado

## ‚öôÔ∏è **Configura√ß√µes Avan√ßadas**

### **Escolher Modelo Espec√≠fico**

Edite `src/services/local_llm_simplifier.py`:

```python
# Para Mistral-7B-Instruct
simplifier = LocalLLMSimplifier("mistralai/Mistral-7B-Instruct-v0.3")

# Para OpenHermes-2.5
simplifier = LocalLLMSimplifier("teknium/OpenHermes-2.5-Mistral-7B")
```

### **Otimizar Performance**

#### **Para GPU NVIDIA**
```python
# Em local_llm_simplifier.py
model_kwargs = {
    "device_map": "auto",
    "torch_dtype": torch.float16,
    "load_in_4bit": True,  # Quantiza√ß√£o 4-bit
}
```

#### **Para CPU Apenas**
```python
# Em local_llm_simplifier.py
model_kwargs = {
    "torch_dtype": torch.float32,
    "device_map": None,
}
```

### **Ajustar Tamanho de Chunks**
```python
# Em epub_processor.py
simplified_content = simplifier.simplify_chapter(
    chapter['content'], 
    simplification_level,
    max_chunk_words=800  # Reduzir para hardware limitado
)
```

### **Personalizar Prompts**

Edite prompts em `src/services/local_llm_simplifier.py`:

```python
self.prompts = {
    1: {
        "system": "Seu prompt personalizado aqui...",
        "user": "Reescreva este trecho..."
    }
}
```

## üîß **Solu√ß√£o de Problemas**

### **Erro: "CUDA out of memory"**
```bash
# Reduzir uso de mem√≥ria GPU
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Ou usar quantiza√ß√£o
load_in_4bit=True
```

### **Erro: "Model too large"**
```python
# Usar modelo menor ou simplificador baseado em regras
from src.services.lightweight_llm import create_lightweight_simplifier
simplifier = create_lightweight_simplifier()
```

### **Processamento Muito Lento**
1. **Reduzir tamanho de chunks**: `max_chunk_words=500`
2. **Usar GPU**: Instalar CUDA e PyTorch GPU
3. **Usar quantiza√ß√£o**: `load_in_4bit=True`
4. **Usar simplificador baseado em regras**

### **Erro: "Protobuf not found"**
```bash
pip install protobuf
```

### **Erro: "Bad Zip file"**
- Arquivo EPUB corrompido
- Tente com outro arquivo EPUB v√°lido

## üìä **Compara√ß√£o de Modelos**

| Modelo | Tamanho | RAM M√≠n. | Qualidade | Velocidade |
|--------|---------|----------|-----------|------------|
| **Mistral-7B** | ~14GB | 16GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **OpenHermes-2.5** | ~14GB | 16GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Baseado em Regras** | <1MB | 4GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

## üîí **Privacidade e Seguran√ßa**

### **Dados Locais**
- Todos os arquivos processados ficam em `/tmp/`
- Nenhum dado √© enviado para servidores externos
- Modelos executam completamente offline

### **Limpeza Autom√°tica**
```python
# Arquivos tempor√°rios s√£o limpos automaticamente
# Para limpeza manual:
rm -rf /tmp/epub_uploads/*
rm -rf /tmp/epub_processed/*
```

## üöÄ **Performance e Benchmarks**

### **Tempos T√≠picos (Livro de 200 p√°ginas)**

| Hardware | Modelo | Tempo |
|----------|--------|-------|
| RTX 4090 + 32GB RAM | Mistral-7B | 5-10 min |
| RTX 3080 + 16GB RAM | OpenHermes | 10-20 min |
| CPU i7 + 16GB RAM | Baseado em Regras | 1-2 min |
| CPU i5 + 8GB RAM | Baseado em Regras | 2-5 min |

## üîÑ **Atualiza√ß√µes e Manuten√ß√£o**

### **Atualizar Modelos**
```bash
# Limpar cache de modelos
rm -rf ~/.cache/huggingface/

# Baixar vers√£o mais recente
python -c "from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('teknium/OpenHermes-2.5-Mistral-7B')"
```

### **Monitorar Uso de Recursos**
```bash
# Monitorar GPU
nvidia-smi

# Monitorar RAM
htop

# Monitorar disco
df -h
```

## üìö **Recursos Adicionais**

### **Links √öteis**
- [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)
- [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)
- [Documenta√ß√£o Transformers](https://huggingface.co/docs/transformers)
- [PyTorch GPU Setup](https://pytorch.org/get-started/locally/)

### **Comunidade**
- Issues e sugest√µes: GitHub Issues
- Discuss√µes: GitHub Discussions
- Contribui√ß√µes: Pull Requests bem-vindos

## üìÑ **Licen√ßa**

Este projeto √© fornecido como est√°, para fins educacionais e de demonstra√ß√£o.

---

**üéâ Aproveite a liberdade de processar seus livros localmente, com privacidade total e sem custos!**

